{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy               as np\n",
    "import tensorflow          as tf\n",
    "import tensorflow_addons   as tfa\n",
    "import matplotlib.pyplot   as plt\n",
    "import pandas              as pd\n",
    "import gudhi               as gd\n",
    "\n",
    "from gudhi.differentiation import *\n",
    "\n",
    "from gudhi.wasserstein     import wasserstein_distance\n",
    "from mpl_toolkits.mplot3d  import Axes3D\n",
    "from sklearn.metrics       import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_devices = tf.config.experimental.list_physical_devices(device_type='CPU')\n",
    "tf.config.experimental.set_visible_devices(devices=my_devices, device_type='CPU')\n",
    "tf.config.experimental.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point cloud application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first experiment, we optimize the point coordinates of an initial random point cloud so that 1-dimensional homology (holes) is maximized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xinit = np.array(np.random.uniform(high=1., low=-1., size=(300,2)), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(Xinit[:,0], Xinit[:,1])\n",
    "plt.title(\"Point cloud at epoch 0\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.Variable(initial_value=Xinit, trainable=True)\n",
    "model = RipsModel(X=X, mel=12., dim=1, card=50)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, dgms = [], []\n",
    "for epoch in range(1000+1):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        # Compute persistence diagram\n",
    "        dgm = model.call()\n",
    "        \n",
    "        # Loss is the opposite of the sum of squares of distances to the diagonal\n",
    "        # plus a penalty for points outside of [0,1]^2\n",
    "        loss = -tf.math.reduce_sum(tf.square(.5*(dgm[:,1]-dgm[:,0]))) + tf.reduce_sum(tf.maximum(tf.abs(X)-1, 0))\n",
    "        \n",
    "    # Compute and apply gradients\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    losses.append(loss.numpy())\n",
    "    dgms.append(dgm)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(model.X.numpy()[:,0], model.X.numpy()[:,1])\n",
    "plt.title(\"Point cloud at epoch \" + str(epoch))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for dg in dgms[:5:-1]:\n",
    "    plt.scatter(dg[:,0], dg[:,1], s=20, marker=\"D\", alpha=0.1)\n",
    "plt.scatter(dgms[-1][:,0], dgms[-1][:,1], s=40, marker=\"D\", c=\"green\")\n",
    "plt.plot([-0.,.2], [-0.,.2])\n",
    "plt.title(\"Optimized persistence diagrams\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D image application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second experiment, we optimize the pixel values of an image so that the artificial noise is removed by minimizing 0-dimensional homology (connected components)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = np.array(pd.read_csv(\"datasets/diff/mnist_test.csv\", header=None, sep=\",\"), dtype=np.float32)\n",
    "idx = np.argwhere(I[:,0] == 8)\n",
    "image = np.reshape(-I[idx[8],1:], [28,28])\n",
    "image = (image-image.min())/(image.max()-image.min())\n",
    "image_clean = np.array(image)\n",
    "image[2:5,2:5]        -= 0.6\n",
    "image[25:27,25:27]    -= 0.6\n",
    "image[25:27,2:5]      -= 0.6\n",
    "image[1:4,24:26]      -= 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(image, cmap='Greys')\n",
    "plt.title(\"Image at epoch 0\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.Variable(initial_value=np.array(image, dtype=np.float32), trainable=True)\n",
    "model = CubicalModel(X, dim=0, card=100)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, dgms, empty = [], [], np.empty([0,2])\n",
    "alpha, gamma = 10., 0.\n",
    "for epoch in range(3000+1):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        # Compute persistence diagram\n",
    "        dgm = model.call()\n",
    "        \n",
    "        # Loss is total persistence plus distance to clean image \n",
    "        loss = alpha * tf.math.reduce_sum(tf.abs(dgm[:,1]-dgm[:,0])) + \\\n",
    "               gamma * tf.math.reduce_sum(tf.math.minimum(tf.abs(model.X), tf.abs(1.-model.X)))\n",
    "                       #tf.math.reduce_sum(tf.square(model.X-image_clean))\n",
    "        \n",
    "    # Compute and apply gradients\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "    losses.append(loss.numpy())\n",
    "    dgms.append(dgm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(model.X.numpy(), cmap='Greys')\n",
    "plt.title(\"Image at epoch \" + str(epoch))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(dgms[0][:,0], dgms[0][:,1], s=40, marker=\"D\", c=\"blue\")\n",
    "for dg in dgms[:-1]:\n",
    "    plt.scatter(dg[:,0], dg[:,1], s=20, marker=\"D\", alpha=0.1)\n",
    "plt.scatter(dgms[-1][:,0], dgms[-1][:,1], s=40, marker=\"D\", c=\"red\")\n",
    "plt.plot([0,1], [0,1])\n",
    "plt.title(\"Optimized persistence diagrams\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D shape application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the third experiment, we optimize the vertex filtration values of a 3D shape so that the corresponding persistence diagram matches another constant one (in the Wasserstein distance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces, coord = np.loadtxt(\"datasets/diff/human_faces\", dtype=float)[:,1:], np.loadtxt(\"datasets/diff/human_coords\", dtype=float)\n",
    "stbase = gd.SimplexTree()\n",
    "for i in range(len(faces)):\n",
    "    stbase.insert(faces[i,:], -1e10)\n",
    "f = open(\"datasets/diff/human_simplextree.txt\", \"w\")\n",
    "for (s,_) in stbase.get_filtration():\n",
    "    for v in s:\n",
    "        f.write(str(v) + \" \")\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Finit = coord[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1\n",
    "fig = plt.figure()\n",
    "cm = plt.cm.get_cmap(\"rainbow\")\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "sc = ax.scatter(coord[::step,0], coord[::step,1], coord[::step,2], c=Finit[::step], s=2, \n",
    "                vmin=0, vmax=.75, cmap=cm)\n",
    "x_limits, y_limits, z_limits = ax.get_xlim3d(), ax.get_ylim3d(), ax.get_zlim3d()\n",
    "x_range, x_middle = abs(x_limits[1] - x_limits[0]), np.mean(x_limits)\n",
    "y_range, y_middle = abs(y_limits[1] - y_limits[0]), np.mean(y_limits)\n",
    "z_range, z_middle = abs(z_limits[1] - z_limits[0]), np.mean(z_limits)\n",
    "plot_radius = 0.5*max([x_range, y_range, z_range])\n",
    "ax.set_xlim3d([x_middle - plot_radius, x_middle + plot_radius])\n",
    "ax.set_ylim3d([y_middle - plot_radius, y_middle + plot_radius])\n",
    "ax.set_zlim3d([z_middle - plot_radius, z_middle + plot_radius])\n",
    "plt.title(\"Function at epoch 0\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = tf.Variable(initial_value=np.array(Finit, dtype=np.float32), trainable=True)\n",
    "model = SimplexTreeModel(F, stbase=\"datasets/diff/human_simplextree.txt\", dim=0, card=50)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, dgms = [], []\n",
    "alpha, gamma = 1., .001\n",
    "for epoch in range(1500+1):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        # Compute persistence diagram\n",
    "        dgm = model.call()\n",
    "    \n",
    "        # Loss is square Wasserstein distance between the current persistence diagram and the target\n",
    "        loss = alpha * tf.square(wasserstein_distance(dgm, tf.constant(np.array([[-.98,-.03]], dtype=np.float32)),\n",
    "                                 order=2, enable_autodiff=True))               \n",
    "    \n",
    "    # Compute and apply gradients\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "    losses.append(loss.numpy())\n",
    "    dgms.append(dgm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1\n",
    "fig = plt.figure()\n",
    "cm = plt.cm.get_cmap(\"rainbow\")\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "sc = ax.scatter(coord[::step,0], coord[::step,1], coord[::step,2], c=F.numpy()[::step], s=1, \n",
    "                vmin=0, vmax=.75, cmap=cm)\n",
    "x_limits, y_limits, z_limits = ax.get_xlim3d(), ax.get_ylim3d(), ax.get_zlim3d()\n",
    "x_range, x_middle = abs(x_limits[1] - x_limits[0]), np.mean(x_limits)\n",
    "y_range, y_middle = abs(y_limits[1] - y_limits[0]), np.mean(y_limits)\n",
    "z_range, z_middle = abs(z_limits[1] - z_limits[0]), np.mean(z_limits)\n",
    "plot_radius = 0.5*max([x_range, y_range, z_range])\n",
    "ax.set_xlim3d([x_middle - plot_radius, x_middle + plot_radius])\n",
    "ax.set_ylim3d([y_middle - plot_radius, y_middle + plot_radius])\n",
    "ax.set_zlim3d([z_middle - plot_radius, z_middle + plot_radius])\n",
    "plt.title(\"Function at epoch \" + str(epoch))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(dgms[0][:,0], dgms[0][:,1], s=40, marker=\"D\", c=\"blue\")\n",
    "for dg in dgms[:-1:2]:\n",
    "    plt.scatter(dg[:,0], dg[:,1], s=20, marker=\"D\", alpha=.1)\n",
    "plt.scatter(dgms[-1][:,0], dgms[-1][:,1], s=40, marker=\"D\", c=\"red\")\n",
    "plt.plot([-1,1], [-1,1])\n",
    "plt.title(\"Optimized persistence diagrams\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the fourth experiment, we optimize the coefficients of a linear regression model so that the persistence diagram of its sublevel sets is empty, except for the three most prominent points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = 50, 100\n",
    "betastar = np.concatenate([np.linspace(-1.,1.,33) for _ in range(3)] + [[-1.]])\n",
    "X = np.random.multivariate_normal(mean=np.zeros(shape=[p]), cov=np.eye(p), size=n)\n",
    "Y = np.matmul(X, betastar) + .05 * np.random.randn(n)\n",
    "X, Y = np.array(X, dtype=np.float32), np.array(Y, dtype=np.float32)\n",
    "stbase = gd.SimplexTree()\n",
    "for i in range(p-1):\n",
    "    stbase.insert([i,i+1], -1e10)\n",
    "f = open(\"datasets/diff/beta_simplextree.txt\", \"w\")\n",
    "for (s,_) in stbase.get_filtration():\n",
    "    for v in s:\n",
    "        f.write(str(v) + \" \")\n",
    "    f.write(\"\\n\")\n",
    "f.close()\n",
    "betainit = np.random.uniform(low=-1., high=1., size=[p])\n",
    "betainit[np.array([25,60,99])] = np.array([-1,-1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(betastar)\n",
    "plt.title(\"Ground-truth coefficients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(betainit)\n",
    "plt.title(\"Coefficients at epoch 0\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = tf.Variable(initial_value=np.array(betainit[:,np.newaxis], dtype=np.float32), trainable=True)\n",
    "model = SimplexTreeModel(beta, stbase=\"datasets/diff/beta_simplextree.txt\", dim=0, card=100)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, dgms, betas = [], [], []\n",
    "alpha, gamma, delta = 1, 1e4, 1e3\n",
    "for epoch in range(100+1):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        # Compute persistence diagram\n",
    "        dgm = model.call()\n",
    "        \n",
    "        # Loss is MSE plus the total persistence except for the three most prominent points\n",
    "        loss = alpha * tf.reduce_sum(tf.square(tf.matmul(X, -beta) - Y)) \\\n",
    "             + gamma * tf.reduce_sum(tf.abs(dgm[2:,1]-dgm[2:,0])) \\\n",
    "             + delta * tf.reduce_sum(tf.abs(beta[1:]-beta[:-1]))\n",
    "             \n",
    "            \n",
    "    # Compute and apply gradients\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    losses.append(loss.numpy())\n",
    "    dgms.append(dgm)\n",
    "    betas.append(beta.numpy()[:,0])\n",
    "\n",
    "beta_stdtop = -betas[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(dgms[0][:,0], dgms[0][:,1], s=40, marker=\"D\", c=\"blue\")\n",
    "for dg in dgms[:-1]:\n",
    "    plt.scatter(dg[:,0], dg[:,1], s=20, marker=\"D\", alpha=.1)\n",
    "plt.scatter(dgms[-1][:,0], dgms[-1][:,1], s=40, marker=\"D\", c=\"red\")\n",
    "plt.plot([-1,1], [-1,1])\n",
    "plt.title(\"Optimized persistence diagrams\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = tf.Variable(initial_value=np.array(betainit[:,np.newaxis], dtype=np.float32), trainable=True)\n",
    "model = SimplexTreeModel(beta, stbase=\"datasets/diff/beta_simplextree.txt\", dim=0, card=100)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, dgms, betas = [], [], []\n",
    "alpha, gamma, delta = 1, 0, 1e3\n",
    "for epoch in range(100+1):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        # Compute persistence diagram\n",
    "        dgm = model.call()\n",
    "        \n",
    "        # Loss is MSE plus the total persistence except for the three most prominent points\n",
    "        loss = alpha * tf.reduce_sum(tf.square(tf.matmul(X, -beta) - Y)) \\\n",
    "             + gamma * tf.reduce_sum(tf.abs(dgm[2:,1]-dgm[2:,0])) \\\n",
    "             + delta * tf.reduce_sum(tf.abs(beta[1:]-beta[:-1]))\n",
    "             \n",
    "            \n",
    "    # Compute and apply gradients\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    losses.append(loss.numpy())\n",
    "    dgms.append(dgm)\n",
    "    betas.append(beta.numpy()[:,0])\n",
    "                 \n",
    "beta_stdtot = -betas[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = tf.Variable(initial_value=np.array(betainit[:,np.newaxis], dtype=np.float32), trainable=True)\n",
    "model = SimplexTreeModel(beta, stbase=\"datasets/diff/beta_simplextree.txt\", dim=0, card=100)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, dgms, betas = [], [], []\n",
    "alpha, gamma, delta = 2, 0, 0\n",
    "for epoch in range(100+1):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        # Compute persistence diagram\n",
    "        dgm = model.call()\n",
    "        \n",
    "        # Loss is MSE plus the total persistence except for the three most prominent points\n",
    "        loss = alpha * tf.reduce_sum(tf.square(tf.matmul(X, -beta) - Y)) \\\n",
    "             + gamma * tf.reduce_sum(tf.abs(dgm[2:,1]-dgm[2:,0])) \\\n",
    "             + delta * tf.reduce_sum(tf.abs(beta[1:]-beta[:-1]))\n",
    "             \n",
    "            \n",
    "    # Compute and apply gradients\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    losses.append(loss.numpy())\n",
    "    dgms.append(dgm)\n",
    "    betas.append(beta.numpy()[:,0])\n",
    "    \n",
    "beta_std = -betas[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(betastar, label=\"ground-truth\")\n",
    "plt.plot(beta_std, label=\"MSE\")\n",
    "plt.plot(beta_stdtot, label=\"MSE+TV\")\n",
    "plt.plot(beta_stdtop, label=\"MSE+TV+TOP\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()\n",
    "print(np.linalg.norm(betastar-beta_std), \n",
    "      np.linalg.norm(betastar-beta_stdtot), \n",
    "      np.linalg.norm(betastar-beta_stdtop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.square(np.matmul(X, beta_std) - Y).sum(), \n",
    "      np.square(np.matmul(X, beta_stdtot) - Y).sum(),\n",
    "      np.square(np.matmul(X, beta_stdtop) - Y).sum())\n",
    "\n",
    "MSEstd, MSEtop, MSEtot = [], [], []\n",
    "for _ in range(1000):\n",
    "    Xnew = np.random.multivariate_normal(mean=np.zeros(shape=[p]), cov=np.eye(p), size=n)\n",
    "    Ynew = np.matmul(Xnew, betastar) #+ np.random.normal(0.,.05,n)\n",
    "    mse_std = np.square(np.matmul(Xnew, beta_std) - Ynew).sum()\n",
    "    mse_tot = np.square(np.matmul(Xnew, beta_stdtot) - Ynew).sum()\n",
    "    mse_top = np.square(np.matmul(Xnew, beta_stdtop) - Ynew).sum()\n",
    "    MSEstd.append(mse_std)\n",
    "    MSEtot.append(mse_tot)\n",
    "    MSEtop.append(mse_top)\n",
    "    \n",
    "plt.figure()\n",
    "plt.boxplot([MSEstd, MSEtot, MSEtop], labels=[\"MSE\", \"MSE+TV\", \"MSE+TV+TOP\"])\n",
    "plt.title(\"MSE on random test sets\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noisy point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, epsilon, nout = 100, .2, 3\n",
    "x, y = np.cos(np.linspace(0,2*np.pi,n)), np.sin(np.linspace(0,2*np.pi,n))\n",
    "np.random.seed(0)\n",
    "ex, ey = np.random.uniform(low=-epsilon,high=epsilon,size=n), np.random.uniform(low=-epsilon,high=epsilon,size=n)\n",
    "np.random.seed(0)\n",
    "outliers = np.random.uniform(low=-.7, high=.7, size=(nout,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(x+ex, y+ey)\n",
    "plt.scatter(outliers[:,0], outliers[:,1])\n",
    "plt.title(\"Point cloud at epoch 0\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = gd.RipsComplex(distance_matrix=pairwise_distances(np.hstack([x[:,np.newaxis],y[:,np.newaxis]])), \n",
    "                    max_edge_length=2.).create_simplex_tree(max_dimension=2)\n",
    "st.persistence()\n",
    "D = np.array(st.persistence_intervals_in_dimension(0), dtype=np.float32)[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(D[:,0], D[:,1])\n",
    "plt.plot([0,2], [0,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xinit = np.array(np.vstack([np.hstack([(x+ex)[:,np.newaxis], (y+ey)[:,np.newaxis]]),outliers]), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.Variable(initial_value=Xinit, trainable=True)\n",
    "model = RipsModel(X=X, mel=2., dim=0, card=150)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, dgms = [], []\n",
    "for epoch in range(100+1):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        # Compute persistence diagram\n",
    "        dgm = model.call()\n",
    "        \n",
    "        # Loss is Wasserstein distance\n",
    "        loss = tf.square(wasserstein_distance(dgm, tf.constant(D), order=2, enable_autodiff=True))\n",
    "        \n",
    "    # Compute and apply gradients\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    losses.append(loss.numpy())\n",
    "    dgms.append(dgm)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(model.X.numpy()[:,0], model.X.numpy()[:,1])\n",
    "plt.title(\"Point cloud at epoch \" + str(epoch))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(dgms[0][:,0], dgms[0][:,1], s=40, marker=\"D\", c=\"blue\")\n",
    "for dg in dgms[:-1]:\n",
    "    plt.scatter(dg[:,0], dg[:,1], s=20, marker=\"D\", alpha=.1)\n",
    "plt.scatter(dgms[-1][:,0], dgms[-1][:,1], s=40, marker=\"D\", c=\"red\")\n",
    "plt.plot([-1,1], [-1,1])\n",
    "plt.title(\"Optimized persistence diagrams\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
