{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topological Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathieu Carri√®re, https://mathieucarriere.github.io/website/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gudhi.cover_complex import MapperComplex, GraphInducedComplex, NerveComplex\n",
    "from gudhi import bottleneck_distance\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will see how to use Gudhi in order to perform topological dimension reduction: we will compute simplicial complex approximations of point cloud / distance matrices. These complexes will be either [Mapper complexes](https://diglib.eg.org/handle/10.2312/SPBG.SPBG07.091-100) or [Graph Induced complexes](http://web.cse.ohio-state.edu/~dey.8/GIC/gic.html). Both complexes use covers of the initial space (such as Voronoi partitions or preimages of filter functions), and use these covers to generate simplicial complexes, either by taking the nerve (Mapper) or by checking the presence of colored cliques (Graph Induced). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gudhi can handle both point clouds and distance matrices. Let's start with a point cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load an example point cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.loadtxt('datasets/human2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X[::2,1], X[::2,0], X[::2,2], s=1)\n",
    "limits = np.array([ax.get_xlim3d(), ax.get_ylim3d(), ax.get_zlim3d()])\n",
    "origin = np.mean(limits, axis=1)\n",
    "radius = 0.5 * np.max(np.abs(limits[:, 1] - limits[:, 0]))\n",
    "x, y, z = origin\n",
    "ax.set_xlim3d([x - radius, x + radius])\n",
    "ax.set_ylim3d([y - radius, y + radius])\n",
    "ax.set_zlim3d([z - radius, z + radius])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the height function to color the complex nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = X[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next provide different configurations for computing cover complexes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph Induced complex with a Voronoi partition with 100 randomly sampled germs and Rips graph obtained with automatic threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cover_complex = GraphInducedComplex(\n",
    "    input_type='point cloud', cover='voronoi', colors=height, mask=0,\n",
    "    graph=\"rips\", rips_threshold=None, N=100, beta=0., C=10,\n",
    "    voronoi_samples=100, \n",
    "    input_name=\"human\", cover_name=\"voronoi\", color_name=\"coord2\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph Induced complex with a preimage partition with automatic resolution and Rips graph obtained with automatic threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cover_complex = GraphInducedComplex(\n",
    "    input_type='point cloud', cover='functional', colors=height, mask=0,\n",
    "    graph=\"rips\", rips_threshold=None, N=100, beta=0., C=10,\n",
    "    filter=height, resolution=None, gain=0.,\n",
    "    input_name=\"human\", cover_name=\"coord2\", color_name=\"coord2\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapper complex with a preimage cover with automatic resolution and hierarchical clustering obtained with automatic threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cover_complex = MapperComplex(\n",
    "    input_type='point cloud', colors=height[:,np.newaxis], mask=0,\n",
    "    clustering=None, N=100, beta=0., C=10,\n",
    "    filters=height[:,np.newaxis], filter_bnds=None, resolutions=None, gains=None,\n",
    "    input_name=\"human\", cover_name=\"coord2\", color_name=\"coord2\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapper complex with a preimage cover with automatic resolution from a 2D function and hierarchical clustering obtained with automatic threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt2d = np.hstack([height[:,np.newaxis],X[:,0:1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cover_complex = MapperComplex(\n",
    "    input_type='point cloud', colors=filt2d, mask=0,\n",
    "    clustering=None, N=100, beta=0., C=10,\n",
    "    filters=filt2d, filter_bnds=None, resolutions=[20,2], gains=None,\n",
    "    input_name=\"human\", cover_name=\"coord2\", color_name=\"coord2\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Distance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can actually process the dataset using only the pairwise distances between points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = pairwise_distances(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This time, color is given by eccentricity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ecc = X.max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cover_complex = GraphInducedComplex(\n",
    "    input_type='distance matrix', cover='functional', colors=ecc, mask=0,\n",
    "    graph=\"rips\", rips_threshold=None, N=100, beta=0., C=10,\n",
    "    filter=ecc, resolution=None, gain=0.,\n",
    "    input_name=\"human\", cover_name=\"coord2\", color_name=\"coord2\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cover_complex = GraphInducedComplex(\n",
    "    input_type='distance matrix', cover='voronoi', colors=ecc, mask=0,\n",
    "    graph=\"rips\", rips_threshold=None, N=100, beta=0., C=10,\n",
    "    voronoi_samples=100, \n",
    "    input_name=\"human\", cover_name=\"coord2\", color_name=\"coord2\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cover_complex = MapperComplex(\n",
    "    input_type='distance matrix', colors=ecc[:,np.newaxis], mask=0,\n",
    "    clustering=None, N=100, beta=0., C=10,\n",
    "    filters=ecc[:,np.newaxis], filter_bnds=None, resolutions=None, gains=None,\n",
    "    input_name=\"human\", cover_name=\"coord2\", color_name=\"coord2\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cover complex can now be computed in a single line of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = cover_complex.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the complex in three different ways with Gudhi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You can use Python package `networkx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = cover_complex.get_networkx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "nx.draw(G, pos=nx.kamada_kawai_layout(G), node_color=[cover_complex.node_info[v][\"colors\"][0] for v in G.nodes()])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. You can create a DOT file that can be processed later with `neato` to produce a PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cover_complex.print_to_dot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!neato -Tpdf human.dot -o human.pdf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. You can create a TXT file that you can process later with our KeplerMapper wrapper to produce a HTML file that you can visualize in browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cover_complex.print_to_txt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /home/mcarrier/Git/gudhi/src/Nerve_GIC/utilities/KeplerMapperVisuFromTxtFile.py -f human.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topological features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will do a variety of post-processing, statistical tasks for the computed cover complex, including, e.g., computing the topological features and/or assessing their stability. For these operations, we will need a set of helper functions, which we define below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cover_complex.data = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.csgraph    import dijkstra, shortest_path, connected_components\n",
    "from scipy.stats             import ks_2samp\n",
    "\n",
    "def find(i, parents):\n",
    "    if parents[i] == i:\n",
    "        return i\n",
    "    else:\n",
    "        return find(parents[i], parents)\n",
    "\n",
    "def union(i, j, parents, f):\n",
    "    if f[i] <= f[j]:\n",
    "        parents[j] = i\n",
    "    else:\n",
    "        parents[i] = j\n",
    "        \n",
    "def compute_topological_features(M, threshold=0.):\n",
    "    \"\"\"\n",
    "    Compute the topological features (connected components, up/down branches, loops) of the 1-skeleton of the cover complex. Connected components and loops are computed with scipy functions, and branches are detected with Union-Find and 0-dimensional persistence of the 1-skeleton.\n",
    "    Parameters:\n",
    "        threshold (float): any topological feature whose size is less than this parameter (relative to the first color function) will be discarded.\n",
    "    Returns:\n",
    "        dgm (list of (dim,(a,b)) tuples): list of feature characteristics. dim is the topological dimension of the feature (0 for CCs and branches, 1 for loops), a,b are the min and max of the first color function along the feature.\n",
    "        bnds (list of lists): list of feature points. Each element of this list is the list of point IDs forming the corresponding feature. \n",
    "    \"\"\"\n",
    "    st = M.simplex_tree\n",
    "    num_nodes = st.num_vertices()\n",
    "    function, namefunc, invnamefunc = {}, {}, {}\n",
    "    nodeID = 0\n",
    "    for (s,_) in st.get_skeleton(0):\n",
    "        namefunc[s[0]] = nodeID\n",
    "        invnamefunc[nodeID] = s[0]\n",
    "        function[s[0]] = M.node_info[s[0]][\"colors\"][0]\n",
    "        nodeID += 1\n",
    "    dgm, bnd = [], []\n",
    "\n",
    "    # connected_components\n",
    "    A = np.zeros([num_nodes, num_nodes])\n",
    "    for (splx,_) in st.get_skeleton(1):\n",
    "        if len(splx) == 2:\n",
    "            A[namefunc[splx[0]], namefunc[splx[1]]] = 1\n",
    "            A[namefunc[splx[1]], namefunc[splx[0]]] = 1\n",
    "    _, ccs = connected_components(A, directed=False)\n",
    "    for ccID in np.unique(ccs):\n",
    "        pts = np.argwhere(ccs == ccID).flatten()\n",
    "        vals = [function[invnamefunc[p]] for p in pts]\n",
    "        if np.abs(min(vals) - max(vals)) >= threshold:\n",
    "            dgm.append((0, (min(vals), max(vals))))\n",
    "            bnd.append([invnamefunc[p] for p in pts])\n",
    "\n",
    "    # loops\n",
    "    G = M.get_networkx()\n",
    "    try:\n",
    "        from networkx import cycle_basis\n",
    "        bndall = cycle_basis(G)\n",
    "        for pts in bndall:\n",
    "            vals = [function[p] for p in pts]\n",
    "            if np.abs(min(vals) - max(vals)) >= threshold:\t\n",
    "                dgm.append((1,(min(vals), max(vals))))\n",
    "                bnd.append(pts)\n",
    "    except ImportError:\n",
    "        print(\"Networkx not found, loops not computed\")\n",
    "        \n",
    "    # branches\n",
    "    for topo_type in [\"downbranch\", \"upbranch\"]:\n",
    "\n",
    "        lfunction = []\n",
    "        for i in range(num_nodes):\n",
    "            lfunction.append(function[invnamefunc[i]])\n",
    "\n",
    "        # upranch is downbranch of opposite function\n",
    "        if topo_type == \"upbranch\":\n",
    "            lfunction = [-f for f in lfunction]\n",
    "\n",
    "        # sort vertices according to function values and compute inverse function \n",
    "        sorted_idxs = np.argsort(np.array(lfunction))\n",
    "        inv_sorted_idxs = np.zeros(num_nodes)\n",
    "        for i in range(num_nodes):\n",
    "            inv_sorted_idxs[sorted_idxs[i]] = i\n",
    "\n",
    "        # go through all vertices in ascending function order\n",
    "        persistence_diag, persistence_set, parents, visited = {}, {}, -np.ones(num_nodes, dtype=np.int32), {}\n",
    "        for i in range(num_nodes):\n",
    "\n",
    "            current_pt = sorted_idxs[i]\n",
    "            neighbors = np.ravel(np.argwhere(A[current_pt,:] == 1))\n",
    "            lower_neighbors = [n for n in neighbors if inv_sorted_idxs[n] <= i] if len(neighbors) > 0 else []\n",
    "\n",
    "            # no lower neighbors: current point is a local minimum\n",
    "            if lower_neighbors == []:\n",
    "                parents[current_pt] = current_pt\n",
    "\n",
    "            # some lower neighbors exist\n",
    "            else:\n",
    "\n",
    "                # find parent pg of lower neighbors with lowest function value\n",
    "                neigh_parents = [find(n, parents) for n in lower_neighbors]\n",
    "                pg = neigh_parents[np.argmin([lfunction[n] for n in neigh_parents])]\n",
    "\n",
    "                # set parent of current point to pg\n",
    "                parents[current_pt] = pg\n",
    "\n",
    "                # for each lower neighbor, we will create a persistence diagram point and corresponding set of nodes\n",
    "                for neighbor in lower_neighbors:\n",
    "\n",
    "                    # get parent pn\n",
    "                    pn = find(neighbor, parents)\n",
    "                    val = lfunction[pn]\n",
    "                    persistence_set[pn] = []\n",
    "\n",
    "                    # we will create persistence set only if parent pn is not local minimum pg\n",
    "                    if pn != pg:\n",
    "                        # go through all strictly lower nodes with parent pn\n",
    "                        for v in sorted_idxs[:i]:\n",
    "                            if find(v, parents) == pn:\n",
    "                                # if it is already part of another persistence set, continue\n",
    "                                try:\n",
    "                                    visited[v]\n",
    "                                # else, mark visited and include it in current persistence set\n",
    "                                except KeyError:\n",
    "                                    visited[v] = True\n",
    "                                    persistence_set[pn].append(v)\n",
    "\n",
    "                        # add current point to persistence set\n",
    "                        persistence_set[pn].append(current_pt)\n",
    "\n",
    "                        # do union and create persistence point corresponding to persistence set if persistence is sufficiently large\n",
    "                        if np.abs(lfunction[pn]-lfunction[current_pt]) >= threshold:\n",
    "                            persistence_diag[pn] = current_pt\n",
    "                            union(pg, pn, parents, lfunction)\n",
    "\n",
    "        for key, val in iter(persistence_diag.items()):\n",
    "            if topo_type == \"downbranch\":\n",
    "                dgm.append((0, (lfunction[key],  lfunction[val])))\n",
    "            elif topo_type == \"upbranch\":\n",
    "                dgm.append((0, (-lfunction[val], -lfunction[key])))\n",
    "            bnd.append([invnamefunc[v] for v in persistence_set[key]])\n",
    "\n",
    "    bnd = [list(b) for b in bnd]\n",
    "    M.persistence_diagram, M.persistence_sets = dgm, bnd \n",
    "    return dgm, bnd\n",
    "\n",
    "def bootstrap_topological_features(M, N):\n",
    "    \"\"\"\n",
    "    Use bootstrap to empirically assess stability of the features. This function computes a distribution of bottleneck distances, that can used afterwards to run tests on each topological feature.\n",
    "    Parameters:\n",
    "        N (int): number of bootstrap iterations.\n",
    "    \"\"\"\n",
    "\n",
    "    dgm = M.persistence_diagram\n",
    "    num_pts, distribution = len(M.data), []\n",
    "    for bootstrap_id in range(N):\n",
    "\n",
    "        print(str(bootstrap_id) + \"th iteration\")\n",
    "\n",
    "        # Randomly select points\n",
    "        idxs = np.random.choice(num_pts, size=num_pts, replace=True)\n",
    "        Xboot = M.data[idxs,:] if M.input_type == \"point cloud\" else M.data[idxs,:][:,idxs]\n",
    "        f_boot, c_boot = M.filters[idxs,:], M.colors[idxs,:]\n",
    "        Mboot = M.__class__(filters=f_boot, filter_bnds=M.filter_bnds, colors=c_boot, \n",
    "                            resolutions=M.resolutions, gains=M.gains, \n",
    "                            input_type=M.input_type, clustering=M.clustering).fit(Xboot)\n",
    "\n",
    "        # Compute the corresponding persistence diagrams\n",
    "        dgm_boot, _ = compute_topological_features(Mboot)\n",
    "\n",
    "        # Compute the bottleneck distance\n",
    "        npts, npts_boot = len(dgm), len(dgm_boot)\n",
    "        D1 = np.array([[dgm[pt][1][0], dgm[pt][1][1]] for pt in range(npts)]) \n",
    "        D2 = np.array([[dgm_boot[pt][1][0], dgm_boot[pt][1][1]] for pt in range(npts_boot)])\n",
    "        bottle = bottleneck_distance(D1, D2)\n",
    "        distribution.append(bottle)\n",
    "        M.distribution = np.sort(distribution)\n",
    "\n",
    "def get_distance_from_confidence_level(M, alpha=.95, complex_type='mapper'):\n",
    "    \"\"\"\n",
    "    Compute the bottleneck distance threshold corresponding to a specific confidence level.\n",
    "    Parameters:\n",
    "        alpha (float): confidence level.\n",
    "    Returns:\n",
    "        distance value (float); each feature whose size is above this distance is sure at confidence level alpha.\n",
    "    \"\"\"\n",
    "    return M.distribution[int(alpha*len(M.distribution))]\n",
    "\n",
    "def get_confidence_level_from_distance(M, distance):\n",
    "    \"\"\"\n",
    "    Compute the confidence level of a specific bottleneck distance threshold.\n",
    "    Parameters:\n",
    "        distance (float): bottleneck distance threshold.\n",
    "    Returns:\n",
    "        confidence level (float); each feature whose size is above the distance threshold is sure at this confidence level.\n",
    "    \"\"\"\n",
    "    return len(np.argwhere(M.distribution <= distance))/len(M.distribution)\n",
    "\n",
    "def get_pvalue(M):\n",
    "    \"\"\"\n",
    "    Compute the p-value, i.e. the opposite of the confidence level of the largest bottleneck distance preserving the topological features.\n",
    "    Returns:\n",
    "        p-value (float)\n",
    "    \"\"\"\n",
    "    distancemin = min([np.abs(pt[1][0]-pt[1][1]) for pt in M.persistence_diagram])\n",
    "    return 1.-M.compute_confidence_from_distance(distancemin)\n",
    "\n",
    "def compute_differential_coordinates(M, nodes=None, features=None, sparse=False):\n",
    "    \"\"\"\n",
    "    Compute the coordinates that best explain a set of nodes VS the rest of the nodes (in the 1-skeleton of the cover complex) with a Kolmogorov-Smirnov test. Only works if input_type is \"point cloud\".\n",
    "    Parameters:\n",
    "        nodes (list of integers): list of nodes to try. For instance, one can take the list of nodes obtained after calling \"compute_topological_features\"\n",
    "        features (list of integers): the coordinates to try. All coordinates are tested if None.\n",
    "        sparse (bool): set to True if your data is sparse and there will be speedup, otherwise use False.\n",
    "    Returns:\n",
    "        features (list of integers): the list of coordinates, ranked from smallest to largest p-values.\n",
    "        p-values (list of float): the corresponding p-values. \n",
    "    \"\"\"\n",
    "    if M.input_type == \"distance matrix\":\n",
    "        print(\"Need coordinates for running differential coordinates!\")\n",
    "        raise\n",
    "\n",
    "    node_info = M.node_info\n",
    "    X = M.data\n",
    "    nodes = [s[0] for s,_ in self.simplex_tree.get_skeleton(0)] if nodes is None else nodes\n",
    "\n",
    "    if features is None:\n",
    "        features = np.arange(X.shape[1])\n",
    "\n",
    "    list_idxs1 = list(np.unique(np.concatenate([node_info[node_name][\"indices\"] for node_name in nodes])))\n",
    "    list_idxs2 = list(set(np.arange(X.shape[0]))-set(list_idxs1))\n",
    "    pvals = []\n",
    "    for f in features:\n",
    "        if sparse:\n",
    "            Xsp = csr_matrix(X)\n",
    "            group1 = np.squeeze(np.array(Xsp[list_idxs1,f].todense()))\n",
    "            group2 = np.squeeze(np.array(Xsp[list_idxs2,f].todense()))\n",
    "        else:\n",
    "            group1, group2 = X[list_idxs1,f], X[list_idxs2,f]\n",
    "        _,pval = ks_2samp(group1, group2)\n",
    "        pvals.append(pval)\n",
    "    pvals = np.array(pvals)\n",
    "    F, P = features[np.argsort(pvals)], np.sort(pvals) \n",
    "    return F, P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various postprocessing one can do on a cover complex. For instance, one can compute the topological features in the complex. For our human shape, the topological features (identified by computing the persistence of the color function on the complex) are the three branches corresponding to the arms and legs (the lower leg correspond to the whole connected component)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgm, bnd = compute_topological_features(cover_complex, threshold=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = cover_complex.get_networkx()\n",
    "plt.figure(figsize=(8,2))\n",
    "for idx, bd in enumerate(bnd):\n",
    "    plt.subplot(1,len(bnd),idx+1)\n",
    "    nx.draw(G, pos=nx.kamada_kawai_layout(G), \n",
    "            node_color=[1 if node in bd else 0 for node in G.nodes()], node_size=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also identify the robust topological features by bootstrapping, and select those associated to 95% confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_topological_features(cover_complex, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = get_distance_from_confidence_level(cover_complex, .95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnd_boot = [b for idx, b in enumerate(bnd) if np.abs(.5 * (dgm[idx][1][1]-dgm[idx][1][0])) >= dist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = cover_complex.get_networkx()\n",
    "plt.figure(figsize=(8,2))\n",
    "for idx, bd in enumerate(bnd_boot):\n",
    "    plt.subplot(1,len(bnd_boot),idx+1)\n",
    "    nx.draw(G, pos=nx.kamada_kawai_layout(G), \n",
    "            node_color=[1 if node in bd else 0 for node in G.nodes()], node_size=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, one can identify the coordinates that best explain a topological feature VS the rest of the complex with a Kolmogorov-Smirnov test. In particular, for each topological feature, we can rank the coordinates with respect to their p-values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, coordinate 2 (height) is the one that best distinguishes the leg from the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_differential_coordinates(cover_complex, nodes=bnd[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, coordinate 0 best explains both arms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_differential_coordinates(cover_complex, nodes=bnd[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_differential_coordinates(cover_complex, nodes=bnd[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
