{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A tutorial and template for *PersLay: a neural network layer for persistence diagrams and new graph topological signatures*.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Author:__ Théo Lacombe, Mathieu Carrière"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ This is an alpha version of PersLay. Do not hesitate to contact the authors for any comment, suggestion, bug, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Outline:__\n",
    "In this notebook:\n",
    "- First, we select a dataset. Two types of datasets are provided by default, either synthetic orbits from dynamical systems, or real-life graph dataset (we also explain how you could use PersLay with your own persistence diagrams).\n",
    "- Then, we generate the persistence diagrams (and other useful informations such as labels, etc.) for the chosen dataset, and optionally visualize them.\n",
    "- We either load a predefined PersLay neural net, or define a neural net that uses some PersLay channels as first layers to handle persistence diagrams. This can be used as a guideline to use PersLay in your own experiments.\n",
    "- We show how to train this neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required Python libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the current version of Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print(\"Current version of your system: (we recommand Python 3.6)\")\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Numpy, Scikit-learn, TensorFlow, PersLay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.svm import *\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, KFold, ShuffleSplit\n",
    "from tensorflow import random_uniform_initializer as rui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gudhi.representations import PerslayModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./utils/')\n",
    "from experiments import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Generate predefined persistence diagrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ Skip this section and go to Section 3 if you already have your own persistence diagrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by choosing the dataset we want to run the experiments on. We suggest the user to start with `\"MUTAG\"` as this dataset is reasonably small (188 graphs with 18 nodes on average). Note that its small size implies a large variability in test accuracies.\n",
    "\n",
    "Available options are:\n",
    "\n",
    "- Orbit datasets: `\"ORBIT5K\"`, `\"ORBIT100K\"`.\n",
    "\n",
    "- Graphs datasets: `\"MUTAG\"`,`\"COX2\"`, `\"DHFR\"`, `\"PROTEINS\"`, `\"NCI1\"`, `\"NCI109\"`,`\"IMDB-BINARY\"`, `\"IMDB-MULTI\"`.\n",
    "\n",
    "__Important note:__ `\"COLLAB\"`,`\"REDDIT5K\"` and `\"REDDIT12K\"` are not available yet (see README.md). Contact the authors for more information.\n",
    "\n",
    "Beware that for the datasets (`\"COLLAB\"`,`\"REDDIT5K\", \"REDDIT12K\", \"ORBIT100K\"`), the files can be quite large (e.g. 3Gb for for `\"ORBIT100K\"`), so that RAM can be limiting, and the time needed to generate the persistence diagrams and run the experiments can be quite long depending on the hardware available. Dataset descriptions are available in Section B of the supplementary material of the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"MUTAG\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we implicitely load our data (saved as `.mat` files for graphs datasets, and generated on-the-fly for orbits datasets---which can take some time for `\"ORBIT100K\"` especially), and then compute the persistence diagrams that will be used in the classification experiment (requires to have `gudhi` installed). For graph datasets, we also generate a series of additional features (see [1]).\n",
    "\n",
    "Running `generate_diag_and_features` will store diagrams, features and labels. Therefore, it is sufficient to run it just once (for each different dataset). Note that for bigger datasets, the computations of these persistence diagrams can be quite long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_diagrams_and_features(dataset, path_dataset=\"./data/MUTAG/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load and preprocess persistence diagrams (to make them PersLay-compatible) and other useful items using the files that we have generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diags_dict, F, L = load_data(dataset, path_dataset=\"./data/MUTAG/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = np.array(F, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualize persistence diagrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to visualise some example of diagrams generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_diagrams(diags_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# (Optional) Use your own persistence diagrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "__Note:__ Skip this section and make sure to go through Section 2 if you want to use the predefined persistence diagrams that we provide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We provide a (hopefully) convenient way to use your own persistence diagrams for a classification task (with some eventual features).\n",
    "\n",
    "Persistence diagrams must be given in the following format:\n",
    "assume you have $N$ observations. For each of them, you build $K$ different persistence diagrams (e.g. persistence diagrams in different homology dimensions, and/or for different filtrations, etc.). \n",
    "\n",
    "Then, you must provide a `diags_dict` variable that is a `dictionary`, whose $K$ keys are the persistence diagram type names (e.g. `\"Rips_dim_0\"`, `\"Cech_dim_1\"`). For each key $k_i$, $1 \\leq i \\leq K$, the corresponding value is a `list` of `np.arrays`, each array encoding a persistence diagram. \n",
    "\n",
    "Note that each list must have the same length $N$ (you need to have the same number of persistence diagrams generated for each list). Note also that you must keep the order (i.e. the first element of each list must correspond to the persistence diagram generated with the first observation, and so on).\n",
    "\n",
    "Below is an example of such a (very simple) dictionary, with two filtrations and two persistence diagrams in each:\n",
    "\n",
    "`diags_tmp = {\"Alpha0\":[np.array([[0.1, 0.2], [0.2, 0.5], [0.3, 0.9]]), np.array([[0.1, 0.4], [0.3, 0.5]]),], \"Alpha1\":[np.array([[0.1, 0.4], [0.2, 0.6], [0.4, 0.9]]), np.array([[0.1, 0.2], [0.5, 0.7], [0.8, 0.9]])]}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### To use your own diagrams, uncomment and complete the following\n",
    "#diags_dict = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now, you must (obviously) provide the labels corresponding to each persistence diagram (be careful to keep the same order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### To use your own labels, uncomment and complete the following\n",
    "#L = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You can use some additional \"standard\" features in your network. These features must be provided as a $N \\times d$ `np.array`, where $N$ is your number of observations (as before) and $d$ is the dimension of your features.\n",
    "\n",
    "If you do not want to use additional features, you must use an empty array of size $(N,0)$, where $N$ is the number of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Uncomment and complete the following line to not include additional features with PersLay.\n",
    "#N = # number of observations\n",
    "#F = np.array([[]]*N)\n",
    "\n",
    "### To use your own features instead, uncomment and complete the following\n",
    "#F = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If your persistence diagrams have not been preprocessed already, we now apply a preprocessing that makes our sets of persistence diagrams compatible with PersLay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Preprocess persistence diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gudhi.representations as tda\n",
    "\n",
    "### Uncomment the following to process your diagrams (necessary)\n",
    "thresh = 500\n",
    "\n",
    "# Whole pipeline\n",
    "tmp = Pipeline([\n",
    "        (\"Selector\",      tda.DiagramSelector(use=True, point_type=\"finite\")),\n",
    "        (\"ProminentPts\",  tda.ProminentPoints(use=True, num_pts=thresh)),\n",
    "        (\"Scaler\",        tda.DiagramScaler(use=True, scalers=[([0,1], MinMaxScaler())])),\n",
    "        (\"Padding\",       tda.Padding(use=True)),\n",
    "                ])\n",
    "\n",
    "prm = {filt: {\"ProminentPts__num_pts\": min(thresh, max([len(dgm) for dgm in diags_dict[filt]]))} \n",
    "       for filt in diags_dict.keys() if max([len(dgm) for dgm in diags_dict[filt]]) > 0}\n",
    "\n",
    "# Apply the previous pipeline on the different filtrations.\n",
    "diags = []\n",
    "for dt in prm.keys():\n",
    "    param = prm[dt]\n",
    "    tmp.set_params(**param)\n",
    "    diags.append(tmp.fit_transform(diags_dict[dt]))\n",
    "\n",
    "# For each filtration, concatenate all diagrams in a single array.\n",
    "D, npts = [], len(diags[0])\n",
    "for dt in range(len(prm.keys())):\n",
    "    D.append(np.array(np.concatenate([diags[dt][i][np.newaxis,:] for i in range(npts)],axis=0),dtype=np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PersLay in a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Load network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, loss, metrics = get_model(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Define your own network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might want to define your own PersLay architecture and to use your own optimizers, losses and/or metrics. To help you with it, we now show the different options regarding the parameters of PersLay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize list of parameters for PersLay. This list will contain the different PersLay channel parameters (there is one channel per filtration/diagram type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perslay_parameters = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perslay_channel = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perslay_channel[\"pweight_train\"] = True\n",
    "perslay_channel[\"layer_train\"]   = True\n",
    "perslay_channel[\"final_model\"]   = \"identity\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Layer type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Choice of layer type, must be one of (see README.md and [1] for details):\n",
    "- `\"Image\"` for a persistence image layer.\n",
    "- `\"PermutationEquivariant\"` for a permutation equivariant layer (as in [2]).\n",
    "- `\"Exponential\"` for an exponential structure element layer (as in [3]).\n",
    "- `\"Rational\"` for a rational structure element layer (as in [3]).\n",
    "- `\"RationalHat\"` for a rational hat structure element layer (as in [3]).\n",
    "- `\"Landscape\"` for a persistence landscape layer.\n",
    "- `\"BettiCurve\"` for a Betti curve layer.\n",
    "- `\"Entropy\"` for a persistence entropy layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "perslay_channel[\"layer\"]           = \"Image\"\n",
    "perslay_channel[\"image_size\"]      = (20, 20)\n",
    "perslay_channel[\"image_bnds\"]      = ((-.001, 1.001), (-.001, 1.001))\n",
    "perslay_channel[\"lvariance_init\"]  = 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "perslay_channel[\"layer\"]           = \"PermutationEquivariant\"\n",
    "perslay_channel[\"lpeq\"]            = [(5, \"max\")]\n",
    "perslay_channel[\"lweight_init\"]    = rui(0.0, 1.0)\n",
    "perslay_channel[\"lbias_init\"]      = rui(0.0, 1.0)\n",
    "perslay_channel[\"lgamma_init\"]     = rui(0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "perslay_channel[\"layer\"]           = \"Exponential\"\n",
    "perslay_channel[\"lnum\"]            = 25\n",
    "perslay_channel[\"lmean_init\"]      = rui(0.0, 1.0)\n",
    "perslay_channel[\"lvariance_init\"]  = rui(3.0, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "perslay_channel[\"layer\"]           = \"Rational\"\n",
    "perslay_channel[\"lnum\"]            = 25\n",
    "perslay_channel[\"lmean_init\"]      = rui(0.0, 1.0)\n",
    "perslay_channel[\"lvariance_init\"]  = rui(3.0, 3.0) \n",
    "perslay_channel[\"lalpha_init\"]     = rui(3.0, 3.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "perslay_channel[\"layer\"]           = \"RationalHat\"\n",
    "perslay_channel[\"lnum\"]            = 25\n",
    "perslay_channel[\"lmean_init\"]      = rui(0.0, 1.0)\n",
    "perslay_channel[\"lr_init\"]         = rui(3.0, 3.0) \n",
    "perslay_channel[\"q\"]               = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "perslay_channel[\"layer\"]           = \"Landscape\"\n",
    "perslay_channel[\"lsample_num\"]     = 100\n",
    "perslay_channel[\"lsample_init\"]    = rui(0.0, 1.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "perslay_channel[\"layer\"]           = \"BettiCurve\"\n",
    "perslay_channel[\"theta\"]           = 10\n",
    "perslay_channel[\"lsample_num\"]     = 100\n",
    "perslay_channel[\"lsample_init\"]    = rui(0.0, 1.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "perslay_channel[\"layer\"]           = \"Entropy\"\n",
    "perslay_channel[\"theta\"]           = 10\n",
    "perslay_channel[\"lsample_num\"]     = 100\n",
    "perslay_channel[\"lsample_init\"]    = rui(0.0, 1.0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Weight function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Choice of the weight function, must be one of:\n",
    "- `\"power\"`, for the distance to the diagonal with some exponent.\n",
    "- `\"grid\"`, for a piecewise-constant function defined with pixel values.\n",
    "- `\"gmix\"`, for a weight function defined as a mixture of Gaussians.\n",
    "- `None`, for a constant weight function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "perslay_channel[\"pweight\"]       = \"power\"\n",
    "perslay_channel[\"pweight_init\"]  = 1.\n",
    "perslay_channel[\"pweight_power\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "perslay_channel[\"pweight\"]       = \"grid\"\n",
    "perslay_channel[\"pweight_size\"]  = [20,20]\n",
    "perslay_channel[\"pweight_bnds\"]  = ((-.001, 1.001), (-.001, 1.001))\n",
    "perslay_channel[\"pweight_init\"]  = rui(1.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "perslay_channel[\"pweight\"]       = \"gmix\"\n",
    "perslay_channel[\"pweight_num\"]   = 3\n",
    "perslay_channel[\"pweight_init\"]  = np.array(np.vstack([np.random.uniform(0.,1.,[2,3]), \n",
    "                                                        5.*np.ones([2,3])]), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "perslay_channel[\"pweight\"]       = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Permutation-invariant operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Choice of permutation invariant operator, must be one of:\n",
    "- `\"sum\"`.\n",
    "- `\"topk\"`, will select the $k$ highest values, specified in `keep`.\n",
    "- `\"max\"`.\n",
    "- `\"mean\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "perslay_channel[\"perm_op\"] = \"sum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "perslay_channel[\"perm_op\"] = \"topk\"\n",
    "perslay_channel[\"keep\"]    = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "perslay_channel[\"perm_op\"] = \"max\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "perslay_channel[\"perm_op\"] = \"mean\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Definition of model and choice of optimizer, loss and metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We use the same channel type for all filtrations and diagram types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "perslay_parameters = [perslay_channel for _ in range(len(D))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "with mirrored_strategy.scope():\n",
    "    \n",
    "    # Final rho network is a simple dense layer to the number of labels \n",
    "    rho = tf.keras.Sequential([tf.keras.layers.Dense(L.shape[1], activation=\"sigmoid\", input_shape=(16039,))])\n",
    "    model = PerslayModel(name=\"PersLay\", diagdim=2, perslay_parameters=perslay_parameters, rho=rho)\n",
    "\n",
    "    # Optimizer is Adam with exponential decay of learning rate and moving average of variables\n",
    "    lr = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=.01, decay_steps=20, decay_rate=0.5)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr, epsilon=1e-4)\n",
    "    optimizer = tfa.optimizers.MovingAverage(optimizer, average_decay=0.9) \n",
    "\n",
    "    # Loss is cross-entropy\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "    # Metric is accuracy\n",
    "    metrics = [tf.keras.metrics.CategoricalAccuracy()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pts = len(D[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single run means using the `evaluate_model` function for training the PersLay architecture once and observing the performance (classification accuracy) on the test set.\n",
    "- For orbit datasets, we suggest to use a 70-30 train-test split, i.e. `test_size = 0.3`.\n",
    "- For graph datasets, we suggest to use a 90-10 train-test split, i.e. `test_size = 0.1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = .1\n",
    "epochs    = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_perm = np.random.permutation(num_pts)\n",
    "train, test = random_perm[:int((1-test_size)*num_pts)], random_perm[int((1-test_size)*num_pts):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, tr, te = evaluate_model(L,F,D,train,test,model,optimizer,loss,metrics,num_epochs=epochs,verbose=0,plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train accuracy = \" + str(tr[1]) + \", test accuracy = \" + str(te[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PersLay as a vectorization method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you just want to learn the best vectorization with PersLay, and then apply a standard classifier, run the following cells to vectorize the diagrams and to define the standard classifiers and their parameters you want to cross-validate on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = model.compute_representations(D).numpy()\n",
    "train_vect, test_vect = np.hstack([vectors[train], F[train]]), np.hstack([vectors[test], F[test]])\n",
    "train_labs, test_labs = np.argmax(L[train], axis=1), np.argmax(L[test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf      = Pipeline([(\"Estimator\", SVC())])\n",
    "clf_prms = [{\"Estimator\":         [RandomForestClassifier()]},\n",
    "            {\"Estimator\":         [SVC()],\n",
    "             \"Estimator__kernel\": [\"linear\", \"rbf\"], \n",
    "             \"Estimator__C\":      [0.1, 1, 10]},\n",
    "            {\"Estimator\":         [AdaBoostClassifier()]}]\n",
    "classifier = GridSearchCV(clf, clf_prms, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(train_vect, train_labs)\n",
    "tr = classifier.score(train_vect, train_labs)\n",
    "te = classifier.score(test_vect,  test_labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train accuracy = \" + str(tr) + \", test accuracy = \" + str(te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is for reproducing the article experiments (10 x 10 folds). As is, it only works for predefined models (obtained with the `get_model` function), but if you want to use your own architecture and persistence diagrams, you just have to compact your model definition (done in Section 5.2) in a single function `get_model` that you can call for each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 10\n",
    "epochs    = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = KFold(n_splits=num_folds, random_state=42, shuffle=True).split(np.empty([num_pts]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size=.1\n",
    "folds = ShuffleSplit(n_splits=num_folds, test_size=test_size, random_state=42).split(np.empty([num_pts]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr, te = [], []\n",
    "for (ir, ie) in folds:\n",
    "    model, optimizer, loss, metrics = get_model(dataset)\n",
    "    _, sr, se = evaluate_model(L, F, D, ir, ie, model, optimizer, loss, metrics, num_epochs=epochs, verbose=0)\n",
    "    tr.append(sr[1])\n",
    "    te.append(se[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train accuracy = \" + str(np.mean(tr)) + \", test accuracy = \" + str(np.mean(te)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[1] _PersLay: A Simple and Versatile Neural Network Layer for Persistence Diagrams._\n",
    "Mathieu Carrière, Frederic Chazal, Yuichi Ike, Théo Lacombe, Martin Royer, Yuhei Umeda.\n",
    "\n",
    "[2] _Deep Sets._\n",
    "Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan Salakhutdinov, Alexander Smola.\n",
    "_Advances in Neural Information Processing Systems 30 (NIPS 2017)_\n",
    "\n",
    "[3] _Learning Representations of Persistence Barcodes._\n",
    "Christoph Hofer, Roland Kwitt, Marc Niethammer.\n",
    "_JMLR (2019)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
