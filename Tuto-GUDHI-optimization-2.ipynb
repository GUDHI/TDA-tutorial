{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import itertools  \n",
    "import numpy                 as np\n",
    "import tensorflow            as tf\n",
    "import matplotlib.pyplot     as plt\n",
    "import pandas                as pd\n",
    "import gudhi                 as gd\n",
    "import gudhi.representations as sktda\n",
    "import sys\n",
    "\n",
    "from gudhi.tensorflow                     import LowerStarSimplexTreeLayer, CubicalLayer, RipsLayer\n",
    "from gudhi.representations.vector_methods import Atol as atol\n",
    "from gudhi.representations.kernel_methods import SlicedWassersteinKernel as swk\n",
    "from gudhi.wasserstein                    import wasserstein_distance\n",
    "from gudhi.representations                import pairwise_persistence_diagram_distances as ppdd\n",
    "from mpl_toolkits.mplot3d                 import Axes3D\n",
    "from scipy.linalg                         import expm\n",
    "from scipy.io                             import loadmat\n",
    "from scipy.sparse                         import csgraph\n",
    "from scipy.linalg                         import eigh\n",
    "from sklearn.base                         import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics                      import pairwise_distances, accuracy_score\n",
    "from sklearn.manifold                     import MDS, LocallyLinearEmbedding, SpectralEmbedding\n",
    "from sklearn.preprocessing                import MinMaxScaler, Normalizer, LabelEncoder\n",
    "from sklearn.pipeline                     import Pipeline, FeatureUnion\n",
    "from sklearn.svm                          import SVC\n",
    "from sklearn.ensemble                     import RandomForestClassifier\n",
    "from sklearn.neighbors                    import KNeighborsClassifier\n",
    "from sklearn.model_selection              import GridSearchCV, KFold, StratifiedKFold\n",
    "from sklearn.cluster                      import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we implement the linear regression experiment, where we recover hidden coefficients using 0-dimensional homology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "n, p = 50, 100\n",
    "betastar = np.concatenate([np.linspace(-1.,1.,33) for _ in range(3)] + [[-1.]])\n",
    "X = np.random.multivariate_normal(mean=np.zeros(shape=[p]), cov=np.eye(p), size=n)\n",
    "\n",
    "Y = np.matmul(X, betastar) + .05 * np.random.randn(n)\n",
    "X, Y = np.array(X, dtype=np.float32), np.array(Y, dtype=np.float32)\n",
    "stbase = gd.SimplexTree()\n",
    "for i in range(p-1):\n",
    "    stbase.insert([i,i+1], -1e10)\n",
    "\n",
    "betainit = np.random.uniform(low=-1., high=1., size=[p])\n",
    "betainit[np.array([25,60,99])] = np.array([-1,-1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(betastar)\n",
    "plt.title('Ground-truth coefficients')\n",
    "plt.savefig('reggt.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(betainit)\n",
    "plt.title('Coefficients at epoch 0')\n",
    "plt.savefig('reginit.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = tf.Variable(initial_value=np.array(betainit[:,np.newaxis], dtype=np.float32), trainable=True)\n",
    "layer = LowerStarSimplexTreeLayer(simplextree=stbase, dimension=0)\n",
    "lr = tf.keras.optimizers.schedules.InverseTimeDecay(initial_learning_rate=1e-5, decay_steps=10, decay_rate=.001)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "sigma = 0.001\n",
    "\n",
    "losses, dgms, betas = [], [], []\n",
    "alpha, gamma, delta = 1, 1e4, 1e3\n",
    "for epoch in range(100+1):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        dgm = layer.call(beta)\n",
    "        loss = alpha * tf.reduce_sum(tf.square(tf.matmul(X, -beta) - Y)) \\\n",
    "             + gamma * tf.reduce_sum(tf.abs(dgm[2:,1]-dgm[2:,0])) \\\n",
    "             + delta * tf.reduce_sum(tf.abs(beta[1:]-beta[:-1]))\n",
    "             \n",
    "    gradients = tape.gradient(loss, [beta])\n",
    "    np.random.seed(epoch)\n",
    "    gradients[0] = gradients[0] + np.random.normal(loc=0., scale=sigma, size=gradients[0].shape)\n",
    "    optimizer.apply_gradients(zip(gradients, [beta]))\n",
    "    losses.append(loss.numpy())\n",
    "    dgms.append(dgm)\n",
    "    betas.append(beta.numpy()[:,0])\n",
    "\n",
    "beta_stdtop = -betas[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig('regloss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(dgms[0][:,0], dgms[0][:,1], s=40, marker='D', c='blue')\n",
    "for dg in dgms[:-1]:\n",
    "    plt.scatter(dg[:,0], dg[:,1], s=20, marker='D', alpha=.1)\n",
    "plt.scatter(dgms[-1][:,0], dgms[-1][:,1], s=40, marker='D', c='red')\n",
    "plt.plot([-1,1], [-1,1])\n",
    "plt.title('Optimized persistence diagrams')\n",
    "plt.savefig('regdg.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = tf.Variable(initial_value=np.array(betainit[:,np.newaxis], dtype=np.float32), trainable=True)\n",
    "layer = LowerStarSimplexTreeLayer(simplextree=stbase, dimension=0)\n",
    "lr = tf.keras.optimizers.schedules.InverseTimeDecay(initial_learning_rate=1e-5, decay_steps=10, decay_rate=.001)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "sigma = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, dgms, betas = [], [], []\n",
    "alpha, gamma, delta = 1, 0, 1e3\n",
    "for epoch in range(100+1):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        dgm = layer.call(beta)\n",
    "        loss = alpha * tf.reduce_sum(tf.square(tf.matmul(X, -beta) - Y)) \\\n",
    "             + gamma * tf.reduce_sum(tf.abs(dgm[2:,1]-dgm[2:,0])) \\\n",
    "             + delta * tf.reduce_sum(tf.abs(beta[1:]-beta[:-1]))\n",
    "             \n",
    "    gradients = tape.gradient(loss, [beta])\n",
    "    np.random.seed(epoch)\n",
    "    gradients[0] = gradients[0] + np.random.normal(loc=0., scale=sigma, size=gradients[0].shape)\n",
    "    optimizer.apply_gradients(zip(gradients, [beta]))\n",
    "    losses.append(loss.numpy())\n",
    "    dgms.append(dgm)\n",
    "    betas.append(beta.numpy()[:,0])\n",
    "                 \n",
    "beta_stdtot = -betas[-1]\n",
    "\n",
    "beta = tf.Variable(initial_value=np.array(betainit[:,np.newaxis], dtype=np.float32), trainable=True)\n",
    "layer = LowerStarSimplexTreeLayer(simplextree=stbase, dimension=0)\n",
    "lr = tf.keras.optimizers.schedules.InverseTimeDecay(initial_learning_rate=1e-5, decay_steps=10, decay_rate=.001)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "sigma = 0.001\n",
    "\n",
    "losses, dgms, betas = [], [], []\n",
    "alpha, gamma, delta = 2, 0, 0\n",
    "for epoch in range(100+1):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        dgm = layer.call(beta)\n",
    "        loss = alpha * tf.reduce_sum(tf.square(tf.matmul(X, -beta) - Y)) \\\n",
    "             + gamma * tf.reduce_sum(tf.abs(dgm[2:,1]-dgm[2:,0])) \\\n",
    "             + delta * tf.reduce_sum(tf.abs(beta[1:]-beta[:-1]))\n",
    "                     \n",
    "    gradients = tape.gradient(loss, [beta])\n",
    "    np.random.seed(epoch)\n",
    "    gradients[0] = gradients[0] + np.random.normal(loc=0., scale=sigma, size=gradients[0].shape)\n",
    "    optimizer.apply_gradients(zip(gradients, [beta]))\n",
    "    losses.append(loss.numpy())\n",
    "    dgms.append(dgm)\n",
    "    betas.append(beta.numpy()[:,0])\n",
    "    \n",
    "beta_std = -betas[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(betastar, label='ground-truth')\n",
    "plt.plot(beta_std, label='MSE')\n",
    "plt.plot(beta_stdtot, label='MSE+TV')\n",
    "plt.plot(beta_stdtop, label='MSE+TV+TOP')\n",
    "plt.legend(loc='upper left')\n",
    "plt.savefig('regafter.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSEstd, MSEtop, MSEtot = [], [], []\n",
    "for s in range(1000):\n",
    "    np.random.seed(s)\n",
    "    Xnew = np.random.multivariate_normal(mean=np.zeros(shape=[p]), cov=np.eye(p), size=n)\n",
    "    Ynew = np.matmul(Xnew, betastar)\n",
    "    mse_std = np.square(np.matmul(Xnew, beta_std) - Ynew).sum()\n",
    "    mse_tot = np.square(np.matmul(Xnew, beta_stdtot) - Ynew).sum()\n",
    "    mse_top = np.square(np.matmul(Xnew, beta_stdtop) - Ynew).sum()\n",
    "    MSEstd.append(mse_std)\n",
    "    MSEtot.append(mse_tot)\n",
    "    MSEtop.append(mse_top)\n",
    "    \n",
    "plt.figure()\n",
    "plt.boxplot([MSEstd, MSEtot, MSEtop], labels=['MSE', 'MSE+TV', 'MSE+TV+TOP'])\n",
    "plt.title('MSE on random test sets')\n",
    "plt.savefig('regmse.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we implement the image experiment, where we remove the noise of an image using 0-dimensional homology. Use `use_reg=True` if you want to use a topological loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_reg = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = np.array(pd.read_csv('../difftda/data/mnist_test.csv', header=None, sep=','), dtype=np.float32)\n",
    "idx = np.argwhere(I[:,0] == 8)\n",
    "image = np.reshape(-I[idx[8],1:], [28,28])\n",
    "image = (image-image.min())/(image.max()-image.min())\n",
    "image_clean = np.array(image)\n",
    "image[2:5,2:5]        -= 0.6\n",
    "image[25:27,25:27]    -= 0.6\n",
    "image[25:27,2:5]      -= 0.6\n",
    "image[1:4,24:26]      -= 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(image, cmap='Greys')\n",
    "plt.title('Image at epoch 0')\n",
    "plt.savefig('imbefore_' + str(use_reg) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.Variable(initial_value=np.array(image, dtype=np.float32), trainable=True)\n",
    "layer = CubicalLayer(dimension=0)\n",
    "lr = tf.keras.optimizers.schedules.InverseTimeDecay(initial_learning_rate=1e-3, decay_steps=10, decay_rate=.01)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "sigma = 0.001\n",
    "\n",
    "losses, dgms, empty = [], [], np.empty([0,2])\n",
    "alpha = 10.\n",
    "gamma = 1. if use_reg else 0\n",
    "for epoch in range(3000+1):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        dgm = layer.call(X)\n",
    "        if use_reg:\n",
    "            loss = alpha * tf.math.reduce_sum(tf.abs(dgm[:,1]-dgm[:,0])) + \\\n",
    "                   gamma * tf.math.reduce_sum(tf.math.minimum(tf.abs(X), tf.abs(1-X)))\n",
    "        else:\n",
    "            loss = alpha * tf.math.reduce_sum(tf.abs(dgm[:,1]-dgm[:,0]))\n",
    "\n",
    "    gradients = tape.gradient(loss, [X])\n",
    "    np.random.seed(epoch)\n",
    "    gradients[0] = gradients[0] + np.random.normal(loc=0., scale=sigma, size=gradients[0].shape)\n",
    "    optimizer.apply_gradients(zip(gradients, [X]))\n",
    "    losses.append(loss.numpy())\n",
    "    dgms.append(dgm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(X.numpy(), cmap='Greys')\n",
    "plt.title('Image at epoch ' + str(epoch))\n",
    "plt.savefig('imafter_' + str(use_reg) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig('imloss_' + str(use_reg) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(dgms[0][:,0], dgms[0][:,1], s=40, marker='D', c='blue')\n",
    "for dg in dgms[:-1]:\n",
    "    plt.scatter(dg[:,0], dg[:,1], s=20, marker='D', alpha=0.1)\n",
    "plt.scatter(dgms[-1][:,0], dgms[-1][:,1], s=40, marker='D', c='red')\n",
    "plt.plot([0,1], [0,1])\n",
    "plt.title('Optimized persistence diagrams')\n",
    "plt.savefig('imdg_' + str(use_reg) + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we implement the point cloud experiment, where we optimize loops in a point cloud using 1-dimensional homology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_reg = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "Xinit = np.array(np.random.uniform(high=1., low=-1., size=(300,2)), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(Xinit[:,0], Xinit[:,1])\n",
    "plt.title('Point cloud at epoch 0')\n",
    "plt.savefig('pcinit_' + str(use_reg) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = tf.Variable(initial_value=Xinit, trainable=True)\n",
    "layer = RipsLayer(maximum_edge_length=12., dimension=1)\n",
    "lr = tf.keras.optimizers.schedules.InverseTimeDecay(initial_learning_rate=1e-1, decay_steps=10, decay_rate=.01)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.)\n",
    "sigma = 0.001\n",
    "\n",
    "losses, dgms = [], []\n",
    "for epoch in range(1000+1):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        dgm = layer.call(X)\n",
    "        if use_reg:\n",
    "            loss = -tf.math.reduce_sum(tf.square(.5*(dgm[:,1]-dgm[:,0]))) + tf.reduce_sum(tf.maximum(tf.abs(X)-1, 0))\n",
    "        else:\n",
    "            loss = -tf.math.reduce_sum(tf.square(.5*(dgm[:,1]-dgm[:,0])))\n",
    "\n",
    "    gradients = tape.gradient(loss, [X])\n",
    "    np.random.seed(epoch)\n",
    "    gradients[0] = gradients[0] + np.random.normal(loc=0., scale=sigma, size=gradients[0].shape)\n",
    "    optimizer.apply_gradients(zip(gradients, [X]))\n",
    "    losses.append(loss.numpy())\n",
    "    dgms.append(dgm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X.numpy()[:,0], X.numpy()[:,1])\n",
    "plt.title('Point cloud at epoch ' + str(epoch))\n",
    "plt.savefig('pcafter_' + str(use_reg) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig('pcloss_' + str(use_reg) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for dg in dgms[:5:-1]:\n",
    "    plt.scatter(dg[:,0], dg[:,1], s=20, marker='D', alpha=0.1)\n",
    "plt.scatter(dgms[-1][:,0], dgms[-1][:,1], s=40, marker='D', c='green')\n",
    "plt.plot([-0.,.2], [-0.,.2])\n",
    "plt.title('Optimized persistence diagrams')\n",
    "plt.savefig('pcdg_' + str(use_reg) + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noisy point cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we implement the noisy point cloud experiment, where we optimize the connected components of a noisy point cloud using 0-dimensional homology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, epsilon, nout = 100, .2, 3\n",
    "x, y = np.cos(np.linspace(0,2*np.pi,n)), np.sin(np.linspace(0,2*np.pi,n))\n",
    "np.random.seed(10)\n",
    "ex, ey = np.random.uniform(low=-epsilon,high=epsilon,size=n), np.random.uniform(low=-epsilon,high=epsilon,size=n)\n",
    "outliers = np.random.uniform(low=-.7, high=.7, size=(nout,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(x+ex, y+ey)\n",
    "plt.scatter(outliers[:,0], outliers[:,1])\n",
    "plt.title('Point cloud at epoch 0')\n",
    "plt.savefig('noisypcinit.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = gd.RipsComplex(distance_matrix=pairwise_distances(np.hstack([x[:,np.newaxis],y[:,np.newaxis]])), max_edge_length=2.).create_simplex_tree(max_dimension=2)\n",
    "st.persistence()\n",
    "D = np.array(st.persistence_intervals_in_dimension(0), dtype=np.float32)[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xinit = np.array(np.vstack([np.hstack([(x+ex)[:,np.newaxis], (y+ey)[:,np.newaxis]]),outliers]), dtype=np.float32)\n",
    "\n",
    "X = tf.Variable(initial_value=Xinit, trainable=True)\n",
    "layer = RipsLayer(maximum_edge_length=2., dimension=0)\n",
    "lr = tf.keras.optimizers.schedules.InverseTimeDecay(initial_learning_rate=1e-1, decay_steps=10, decay_rate=.01)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "sigma = 0.001\n",
    "\n",
    "losses, dgms = [], []\n",
    "for epoch in range(100+1):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        dgm = layer.call(X)\n",
    "        loss = tf.square(wasserstein_distance(dgm, tf.constant(D), order=2, enable_autodiff=True))\n",
    "        \n",
    "    gradients = tape.gradient(loss, [X])\n",
    "    np.random.seed(epoch)\n",
    "    gradients[0] = gradients[0] + np.random.normal(loc=0., scale=sigma, size=gradients[0].shape)\n",
    "    optimizer.apply_gradients(zip(gradients, [X]))\n",
    "    losses.append(loss.numpy())\n",
    "    dgms.append(dgm)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X.numpy()[:,0], X.numpy()[:,1])\n",
    "plt.title('Point cloud at epoch ' + str(epoch))\n",
    "plt.savefig('noisypcafter.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig('noisypcloss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(dgms[0][:,0], dgms[0][:,1], s=40, marker='D', c='blue')\n",
    "for dg in dgms[:-1]:\n",
    "    plt.scatter(dg[:,0], dg[:,1], s=20, marker='D', alpha=.1)\n",
    "plt.scatter(dgms[-1][:,0], dgms[-1][:,1], s=40, marker='D', c='red')\n",
    "plt.plot([-1,1], [-1,1])\n",
    "plt.title('Optimized persistence diagrams')\n",
    "plt.savefig('noisypcdg.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we implement the 3D shape experiment, where we optimize the values on a 3D shape using 0-dimensional homology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces, coord = np.loadtxt('../difftda/data/human_faces', dtype=float)[:,1:], np.loadtxt('../difftda/data/human_coords', dtype=float)\n",
    "stbase = gd.SimplexTree()\n",
    "for i in range(len(faces)):\n",
    "    stbase.insert(faces[i,:], -1e10)\n",
    "Finit = coord[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1\n",
    "fig = plt.figure()\n",
    "cm = plt.cm.get_cmap('rainbow')\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "sc = ax.scatter(coord[::step,0], coord[::step,1], coord[::step,2], c=Finit[::step], s=2, \n",
    "                vmin=0, vmax=.75, cmap=cm)\n",
    "x_limits, y_limits, z_limits = ax.get_xlim3d(), ax.get_ylim3d(), ax.get_zlim3d()\n",
    "x_range, x_middle = abs(x_limits[1] - x_limits[0]), np.mean(x_limits)\n",
    "y_range, y_middle = abs(y_limits[1] - y_limits[0]), np.mean(y_limits)\n",
    "z_range, z_middle = abs(z_limits[1] - z_limits[0]), np.mean(z_limits)\n",
    "plot_radius = 0.5*max([x_range, y_range, z_range])\n",
    "ax.set_xlim3d([x_middle - plot_radius, x_middle + plot_radius])\n",
    "ax.set_ylim3d([y_middle - plot_radius, y_middle + plot_radius])\n",
    "ax.set_zlim3d([z_middle - plot_radius, z_middle + plot_radius])\n",
    "plt.title('Function at epoch 0')\n",
    "plt.savefig('d3sinit.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = tf.Variable(initial_value=np.array(Finit, dtype=np.float32), trainable=True)\n",
    "layer = LowerStarSimplexTreeLayer(simplextree=stbase, dimension=0)\n",
    "lr = tf.keras.optimizers.schedules.InverseTimeDecay(initial_learning_rate=1e-1, decay_steps=10, decay_rate=.01)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "sigma = 0.001\n",
    "\n",
    "losses, dgms = [], []\n",
    "alpha, gamma = 1., .001\n",
    "for epoch in range(3000+1):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        dgm = layer.call(F)\n",
    "        loss = alpha * tf.square(wasserstein_distance(dgm, tf.constant(np.array([[-.98,-.03]], dtype=np.float32)), order=2, enable_autodiff=True))               \n",
    "    \n",
    "    gradients = tape.gradient(loss, [F])\n",
    "    np.random.seed(epoch)\n",
    "    gradients = [tf.convert_to_tensor(gradients[0]) + np.random.normal(loc=0., scale=sigma, size=gradients[0].dense_shape)]\n",
    "    optimizer.apply_gradients(zip(gradients, [F]))\n",
    "    losses.append(loss.numpy())\n",
    "    dgms.append(dgm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1\n",
    "fig = plt.figure()\n",
    "cm = plt.cm.get_cmap('rainbow')\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "sc = ax.scatter(coord[::step,0], coord[::step,1], coord[::step,2], c=F.numpy()[::step], s=1, \n",
    "                vmin=0, vmax=.75, cmap=cm)\n",
    "x_limits, y_limits, z_limits = ax.get_xlim3d(), ax.get_ylim3d(), ax.get_zlim3d()\n",
    "x_range, x_middle = abs(x_limits[1] - x_limits[0]), np.mean(x_limits)\n",
    "y_range, y_middle = abs(y_limits[1] - y_limits[0]), np.mean(y_limits)\n",
    "z_range, z_middle = abs(z_limits[1] - z_limits[0]), np.mean(z_limits)\n",
    "plot_radius = 0.5*max([x_range, y_range, z_range])\n",
    "ax.set_xlim3d([x_middle - plot_radius, x_middle + plot_radius])\n",
    "ax.set_ylim3d([y_middle - plot_radius, y_middle + plot_radius])\n",
    "ax.set_zlim3d([z_middle - plot_radius, z_middle + plot_radius])\n",
    "plt.title('Function at epoch ' + str(epoch))\n",
    "plt.savefig('d3safter.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig('d3sloss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(dgms[0][:,0], dgms[0][:,1], s=40, marker='D', c='blue')\n",
    "for dg in dgms[:-1:2]:\n",
    "    plt.scatter(dg[:,0], dg[:,1], s=20, marker='D', alpha=.1)\n",
    "plt.scatter(dgms[-1][:,0], dgms[-1][:,1], s=40, marker='D', c='red')\n",
    "plt.plot([-1,1], [-1,1])\n",
    "plt.title('Optimized persistence diagrams')\n",
    "plt.savefig('d3sdg.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we implement the filter selection experiment, where we optimize a filter for classification using 0-dimensional homology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiltrationSelector(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, use=False, index_filt=0):\n",
    "        self.use, self.index_filt = use, index_filt\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.use:\n",
    "            Xfit = [D[self.index_filt] for D in X]\n",
    "        else:\n",
    "            Xfit = X\n",
    "        return Xfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset                   = '23'\n",
    "step                      = 10\n",
    "initial_learning_rate     = 0.001\n",
    "batch_size                = 100\n",
    "num_epochs                = 20\n",
    "numdir                    = 10\n",
    "hcard                     = 20\n",
    "hdim                      = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetainit = np.linspace(-np.pi/2, np.pi/2, num=numdir)\n",
    "Cinit = np.array([0., np.pi/2])\n",
    "num_filts = len(Cinit)\n",
    "X = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1, l2 = int(dataset[0]), int(dataset[1])\n",
    "tridxs1, tridxs2 = np.argwhere(X[0][1] == l1).ravel()[::step], np.argwhere(X[0][1] == l2).ravel()[::step]\n",
    "teidxs1, teidxs2 = np.argwhere(X[1][1] == l1).ravel()[::step], np.argwhere(X[1][1] == l2).ravel()[::step]\n",
    "IMG = [X[0][0][j] for j in tridxs1]    + [X[0][0][j] for j in tridxs2]    + [X[1][0][j] for j in teidxs1]    + [X[1][0][j] for j in teidxs2]\n",
    "LAB = [0 for _ in range(len(tridxs1))] + [1 for _ in range(len(tridxs2))] + [0 for _ in range(len(teidxs1))] + [1 for _ in range(len(teidxs2))]\n",
    "ntrain = len(tridxs1) + len(tridxs2)\n",
    "ntot = len(tridxs1) + len(tridxs2) + len(teidxs1) + len(teidxs2)\n",
    "train_idxs, test_idxs = np.arange(0, ntrain), np.arange(ntrain, ntot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DGMb = []\n",
    "for pdi in range(num_filts):\n",
    "    DGMi = []\n",
    "    for j, img in enumerate(IMG):\n",
    "        inds = np.argwhere(img > 0)\n",
    "        I = np.inf * np.ones(img.shape)\n",
    "        for i in range(len(inds)):\n",
    "            val = np.cos(Cinit[pdi])*inds[i,0] + np.sin(Cinit[pdi])*inds[i,1]\n",
    "            I[inds[i,0], inds[i,1]] = val\n",
    "        ccb = gd.CubicalComplex(top_dimensional_cells=I)\n",
    "        ccb.persistence()\n",
    "        dgmb = ccb.persistence_intervals_in_dimension(hdim)\n",
    "        DGMi.append(dgmb)\n",
    "\n",
    "        if j == 0:\n",
    "            vm, vM = min(list(I[I!=np.inf].flatten())), max(list(I[I!=np.inf].flatten()))\n",
    "            plt.figure()\n",
    "            plt.imshow(I, vmin=vm, vmax=vM)\n",
    "            plt.colorbar()\n",
    "            plt.savefig(str(j) + '_' + '{:.2f}'.format(Cinit[pdi]) + '.png')\n",
    "\n",
    "    DGMb.append(DGMi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filts = len(DGMb)\n",
    "num_diags = len(DGMb[0])\n",
    "DGMb = [[DGMb[f][i] for f in range(num_filts)] for i in range(num_diags)]\n",
    "\n",
    "train_dgmbs, test_dgmbs = [DGMb[i] for i in train_idxs], [DGMb[i] for i in test_idxs]\n",
    "train_labs,  test_labs  = [LAB[i]  for i in train_idxs], [LAB[i]  for i in test_idxs]\n",
    "le = LabelEncoder().fit(train_labs + test_labs)\n",
    "train_labs, test_labs = le.transform(train_labs), le.transform(test_labs)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('Feats', FeatureUnion([  ('Pipe' + str(nf), Pipeline([('Selector',  FiltrationSelector(index_filt=nf)),\n",
    "                                                           ('Separator', sktda.DiagramSelector(limit=np.inf, point_type='finite')),\n",
    "                                                           ('TDA',       sktda.Landscape())\n",
    "                                                          ])) for nf in range(num_filts)\n",
    "                           ])),\n",
    "    ('Estimator', RandomForestClassifier())\n",
    "])\n",
    "param = {'Feats__Pipe0__Selector__use':        True,\n",
    "         'Feats__Pipe0__Separator__use':       True,\n",
    "         'Feats__Pipe0__TDA__resolution':      50,\n",
    "         'Feats__Pipe0__TDA__num_landscapes':  5}\n",
    "for nf in range(num_filts-1):\n",
    "    newparam = {'Feats__Pipe' + str(nf+1) + '__Selector__use':        True,\n",
    "                'Feats__Pipe' + str(nf+1) + '__Separator__use':       True,\n",
    "                'Feats__Pipe' + str(nf+1) + '__TDA__resolution':      50,\n",
    "                'Feats__Pipe' + str(nf+1) + '__TDA__num_landscapes':  5}\n",
    "    param.update(newparam)\n",
    "param['Estimator__random_state'] = 0\n",
    "\n",
    "modelb = pipe.set_params(**param)\n",
    "modelb.fit(train_dgmbs, train_labs)\n",
    "trb = modelb.score(train_dgmbs, train_labs)\n",
    "teb = modelb.score(test_dgmbs,  test_labs)\n",
    "train_imgs = np.vstack([IMG[i].flatten()[np.newaxis,:] for i in train_idxs]) \n",
    "test_imgs  = np.vstack([IMG[i].flatten()[np.newaxis,:] for i in test_idxs])\n",
    "rf = RandomForestClassifier().fit(train_imgs, train_labs)\n",
    "trbb = rf.score(train_imgs, train_labs)\n",
    "tebb = rf.score(test_imgs,  test_labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = tf.Variable(initial_value=np.array(Cinit, dtype=np.float32), trainable=True)\n",
    "thetas = tf.Variable(initial_value=np.array(thetainit, dtype=np.float32), trainable=False)\n",
    "lr = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps=1e5, decay_rate=0.99, staircase=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "batch_size = min(batch_size, len(train_idxs))\n",
    "\n",
    "losses, coeffs, accs = [], [], []\n",
    "for epoch in range(num_epochs+1):\n",
    "    \n",
    "    np.random.seed(int(1e2*epoch))\n",
    "    batch = np.random.choice(train_idxs, batch_size, replace=False)\n",
    "    batch_labs = [LAB[i] for i in batch]\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        dists = []\n",
    "        for nf in range(num_filts):\n",
    "            \n",
    "            dgms = []\n",
    "            for i in batch:\n",
    "                img = IMG[i]\n",
    "                inds = np.argwhere(img > 0)\n",
    "                IX, IY = 1e3 * np.ones(img.shape), 1e3 * np.ones(img.shape)\n",
    "                for k in range(len(inds)):\n",
    "                    IX[inds[k,0], inds[k,1]] = inds[k,0]\n",
    "                    IY[inds[k,0], inds[k,1]] = inds[k,1]\n",
    "                II = tf.math.cos(C[nf])*IX + tf.math.sin(C[nf])*IY\n",
    "                dgm = CubicalLayer(dimension=hdim).call(II)\n",
    "                dgms.append(dgm)\n",
    "        \n",
    "            dgms = [tf.pad(d,tf.constant([[0,hcard-d.shape[0]],[0,0]])) for d in dgms]\n",
    "            proj_dgms = tf.linalg.matmul(tf.concat(dgms,axis=0), .5*tf.ones([2,2], tf.float32))\n",
    "            dgms_big = tf.concat([tf.reshape(tf.concat([\n",
    "                dgm, proj_dgms[:hcard*idg], proj_dgms[hcard*(idg+1):]\n",
    "            ], axis=0), [-1,2,1,1]) for idg, dgm in enumerate(dgms)], axis=2)\n",
    "            cosines, sines = tf.math.cos(thetas), tf.math.sin(thetas)\n",
    "            vecs = tf.concat([tf.reshape(cosines,[1,1,1,-1]), tf.reshape(sines,[1,1,1,-1])], axis=1)\n",
    "            theta_projs = tf.sort(tf.math.reduce_sum(tf.math.multiply(dgms_big,vecs), axis=1), axis=0)\n",
    "            t1 = tf.reshape(theta_projs, [hcard*batch_size,-1,1,numdir])\n",
    "            t2 = tf.reshape(theta_projs, [hcard*batch_size,1,-1,numdir])\n",
    "            dists.append(tf.math.reduce_mean(tf.math.reduce_sum(tf.math.abs(t1-t2), axis=0), axis=2))\n",
    "\n",
    "        loss = 0.\n",
    "        classes = np.unique(batch_labs)\n",
    "        for l in classes:\n",
    "            lidxs = np.argwhere(np.array(batch_labs) == l).ravel()\n",
    "            idxs1 = list(itertools.product(lidxs, lidxs))\n",
    "            idxs2 = list(itertools.product(lidxs, range(batch_size)))\n",
    "            for nf in range(num_filts):\n",
    "                cost1 = tf.math.reduce_sum(tf.gather_nd(dists[nf], idxs1))\n",
    "                cost2 = tf.math.reduce_sum(tf.gather_nd(dists[nf], idxs2))\n",
    "                loss += cost1 / cost2\n",
    "    \n",
    "    gradients = tape.gradient(loss, [C]) \n",
    "    optimizer.apply_gradients(zip(gradients, [C]))\n",
    "    curr_coeff = C.numpy().flatten()\n",
    "    losses.append(loss.numpy())\n",
    "    coeffs.append(curr_coeff)\n",
    "    if epoch % 10 == 0:\n",
    "\n",
    "        final_coeff = C.numpy()\n",
    "\n",
    "        DGM = []\n",
    "        for nf in range(num_filts):\n",
    "            DGMi = []\n",
    "            for i, img in enumerate(IMG):\n",
    "                inds = np.argwhere(img > 0)\n",
    "                I = np.inf * np.ones(img.shape)\n",
    "                for i in range(len(inds)):\n",
    "                    val = np.cos(final_coeff[nf])*inds[i,0] + np.sin(final_coeff[nf])*inds[i,1]\n",
    "                    I[inds[i,0], inds[i,1]] = val\n",
    "                cc = gd.CubicalComplex(top_dimensional_cells=I)\n",
    "                cc.persistence()\n",
    "                dgm = cc.persistence_intervals_in_dimension(hdim)\n",
    "                DGMi.append(dgm)\n",
    "            DGM.append(DGMi)\n",
    "\n",
    "        num_filts = len(DGM)\n",
    "        num_diags = len(DGM[0])\n",
    "        DGM = [[DGM[f][i] for f in range(num_filts)] for i in range(num_diags)]\n",
    "        train_dgms, test_dgms = [DGM[i] for i in train_idxs], [DGM[i] for i in test_idxs]\n",
    "        model = pipe.set_params(**param)\n",
    "        model.fit(train_dgms, train_labs)\n",
    "        tr = model.score(train_dgms, train_labs)\n",
    "        te = model.score(test_dgms,  test_labs)\n",
    "\n",
    "        accs.append([trbb, tebb, trb, teb, tr, te])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig('fltloss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot([accs[i][0] for i in range(len(accs))], label='baseline-tr')\n",
    "plt.plot([accs[i][1] for i in range(len(accs))], label='baseline-te')\n",
    "plt.plot([accs[i][2] for i in range(len(accs))], label='w/o optim-tr')\n",
    "plt.plot([accs[i][3] for i in range(len(accs))], label='w/o optim-te')\n",
    "plt.plot([accs[i][4] for i in range(len(accs))], label='optim-tr')\n",
    "plt.plot([accs[i][5] for i in range(len(accs))], label='optim-te')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "plt.savefig('fltacc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CS = np.array(coeffs)\n",
    "plt.figure()\n",
    "for c in range(CS.shape[1]):\n",
    "    plt.plot(CS[:,c])\n",
    "plt.savefig('fltcoeff.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
